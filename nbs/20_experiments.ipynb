{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Class to retrieve comet experiment data.\n",
    "output-file: experiments.html\n",
    "title: Experiments class\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, json, csv\n",
    "from comet_ml import API, APIExperiment, start\n",
    "from pct.hierarchy import PCTHierarchy  \n",
    "from comet_ml.query import Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CometExperimentManager:\n",
    "    def __init__(self, api_key: str = None, workspace: str = None):\n",
    "        self.api = API(api_key)\n",
    "        self.workspace = workspace\n",
    "\n",
    "    def get_all_artifacts_indexed(self):\n",
    "        \"\"\"Retrieve all artifacts and sort them by source experiment key.\"\"\"\n",
    "        filename = '/tmp/artifacts/artifacts_results.json'\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r') as file:\n",
    "                return json.load(file)\n",
    "            \n",
    "        artifacts = self.api.get_artifact_list(workspace=self.workspace)\n",
    "        experiment = start(workspace=self.workspace)\n",
    "        results = {}\n",
    "        for artifact_dict in artifacts['artifacts']:\n",
    "            id = artifact_dict['name']\n",
    "            try:\n",
    "                logged_artifact = experiment.get_artifact(id)\n",
    "                print(logged_artifact.source_experiment_key, id)\n",
    "                results[logged_artifact.source_experiment_key] = id\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving artifact {id}: {e}\")\n",
    "\n",
    "        # Save results to a file\n",
    "        with open(filename, \"w\") as file:\n",
    "            json.dump(results, file, indent=4)\n",
    "        experiment.end()\n",
    "        return results\n",
    "\n",
    "    def get_experiments_by_metrics(self, project_name: str = None, score_threshold: float = None, reward_threshold: float = None, max : bool = False):\n",
    "        \"\"\"\n",
    "        Retrieve experiments for a project where the metric 'score' is less than\n",
    "        score_threshold and 'reward_avg' is greater than or equal to reward_threshold.\n",
    "        \"\"\"\n",
    "        if max:\n",
    "            experiments = self.api.query(self.workspace, project_name, \n",
    "                                       (Metric(\"score\") > score_threshold) & \n",
    "                                       (Metric(\"reward_avg\") > reward_threshold))\n",
    "        else:\n",
    "            experiments = self.api.query(self.workspace, project_name, \n",
    "                                       (Metric(\"score\") < score_threshold) & \n",
    "                                       (Metric(\"reward_avg\") > reward_threshold))\n",
    " \n",
    "        return experiments\n",
    "\n",
    "    def get_artifact_name(self, experiment: APIExperiment = None):\n",
    "        \"\"\"Retrieve the name of an artifact from an experiment.\"\"\"\n",
    "        artifacts = experiment.get_artifacts()\n",
    "        return artifacts[0]['artifact_name'] if artifacts else None\n",
    "\n",
    "    def download_and_run_artifact(self, artifact_name: str = None, seeds: list[int] = None):\n",
    "        \"\"\"\n",
    "        Download an artifact to '/tmp/artifacts/' and run PCTHierarchy.run_from_file\n",
    "        with the artifact filename, returning the score value for each run.\n",
    "        \"\"\"\n",
    "        download_path = f\"/tmp/artifacts/\"\n",
    "        os.makedirs(os.path.dirname(download_path), exist_ok=True)\n",
    " \n",
    "        full_path = os.path.join(download_path, artifact_name)\n",
    "        if os.path.exists(full_path):\n",
    "            pass\n",
    "        else:\n",
    "            experiment = start(workspace=self.workspace)\n",
    "            logged_artifact  = experiment.get_artifact(artifact_name)\n",
    "            # print(logged_artifact.source_experiment_key)\n",
    "            # local_artifact = logged_artifact.download(download_path)\n",
    "            logged_artifact.download(download_path)\n",
    "            # filename = f\"{local_artifact.download_local_path}{artifact_name}\"\n",
    "            experiment.end()\n",
    "    \n",
    "        rewards = []\n",
    "        for seed in seeds:\n",
    "            hierarchy, score = PCTHierarchy.run_from_file(full_path, seed=seed)\n",
    "            metrics = hierarchy.get_environment().get_metrics()\n",
    "            # print(f'Score={score:0.3f} {metrics}')\n",
    "            rewards.append(metrics['reward'])\n",
    "\n",
    "\n",
    "\n",
    "        return rewards\n",
    "\n",
    "    def get_original_metrics(self, experiment: APIExperiment = None):\n",
    "        \"\"\"Retrieve the metrics for an experiment.\"\"\"\n",
    "        metrics = {}\n",
    "        metrics['score'] = eval(experiment.get_metrics(\"score\")[0]['metricValue'])\n",
    "        hyperparameters = experiment.get_parameters_summary()\n",
    "\n",
    "        for param in hyperparameters:\n",
    "            if param['name'] == 'mode':\n",
    "                metrics['mode'] = param['valueCurrent']\n",
    "                break\n",
    "\n",
    "        metrics['name'] = experiment.name\n",
    "        return metrics\n",
    "\n",
    "    def run_experiments_and_record_results(self, project_name: str = None, experiments: list[APIExperiment] = None, artifact_results: dict = None, num_runs: int = 0, output_csv: str = None):\n",
    "        \"\"\"\n",
    "        Run each experiment for a given project a specified number of times and record the score and the\n",
    "        number of times the reward is 100, -100, or something else. Save results to a CSV file.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for experiment in experiments:\n",
    "            try:\n",
    "                artifact_name = artifact_results[experiment.id]\n",
    "            except KeyError:\n",
    "                print(f\"WARNING: Artifact not found for experiment {experiment.id}\")\n",
    "                continue\n",
    "\n",
    "            metrics = self.get_original_metrics(experiment)\n",
    "\n",
    "            print(f\"Running experiment {experiment.id} in project {project_name} with artifact {artifact_name}\")\n",
    "            if not artifact_name:\n",
    "                continue\n",
    "\n",
    "            rewards = self.download_and_run_artifact(artifact_name, seeds=range(num_runs))\n",
    "            reward_counts = {'100': 0, '-100': 0, 'other': 0}\n",
    "\n",
    "            for reward in rewards:\n",
    "                if reward == 100:\n",
    "                    reward_counts['100'] += 1\n",
    "                elif reward == -100:\n",
    "                    reward_counts['-100'] += 1\n",
    "                else:\n",
    "                    reward_counts['other'] += 1\n",
    "            print(f\"Rewards: {reward_counts}\")\n",
    "            results.append({\n",
    "                'name': metrics['name'],\n",
    "                'score': round(metrics['score'], 5),\n",
    "                'mode': metrics['mode'],\n",
    "                'reward_100': reward_counts['100'],\n",
    "                'reward_-100': reward_counts['-100'],\n",
    "                'reward_other': reward_counts['other'],\n",
    "                'experiment_key': '/'.join((\"https://www.comet.com\", self.workspace, project_name, experiment.id)),\n",
    "                'artifact_name': artifact_name\n",
    "            })\n",
    "        # Sort results by 'reward_100' in descending order, then by 'reward_other' in descending order\n",
    "        results.sort(key=lambda x: (-x['reward_100'], -x['reward_other']))\n",
    "        # Save results to CSV\n",
    "        with open(output_csv, mode='w', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=['name', 'score', 'mode', 'reward_100', 'reward_-100', 'reward_other', 'experiment_key', 'artifact_name'])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(results)\n",
    "\n",
    "    def get_workspace_projects(self):\n",
    "        \"\"\"\n",
    "        Get all projects from the current workspace.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of project names in the workspace\n",
    "        \"\"\"\n",
    "        api = API()\n",
    "        projects = api.get_projects(workspace=self.workspace)\n",
    "        return projects\n",
    "        # return [project['name'] for project in projects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m no such metrics: 'reward_avg'; ignoring query, returning no matches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered experiments: []\n"
     ]
    }
   ],
   "source": [
    "#|gui\n",
    "# Initialize the manager\n",
    "workspace = 'lunarlandercontinuous-v2'\n",
    "project_name = 'refinputs-smooth'\n",
    "manager = CometExperimentManager(workspace=workspace)\n",
    "\n",
    "# Test get_all_artifacts_sorted\n",
    "artifact_results = manager.get_all_artifacts_indexed()\n",
    "# print(\"Artifacts sorted by source experiment key:\", artifacts)\n",
    "\n",
    "# Test get_experiments_by_metrics\n",
    "experiments = manager.get_experiments_by_metrics(project_name=project_name, score_threshold=0.05, reward_threshold=10.0)\n",
    "print(\"Filtered experiments:\", experiments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|gui\n",
    "# import random\n",
    "\n",
    "# Test run_experiments_and_record_results\n",
    "if experiments:\n",
    "    output_csv = \"/tmp/artifacts/experiment_results.csv\"\n",
    "    manager.run_experiments_and_record_results(experiments=experiments, project_name=project_name, artifact_results=artifact_results, num_runs=2, output_csv=output_csv)\n",
    "    print(f\"Results saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
