{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Class to retrieve comet experiment data.\n",
    "output-file: experiments.html\n",
    "title: Experiments class\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, json, csv\n",
    "from comet_ml import API, APIExperiment, start\n",
    "from pct.hierarchy import PCTHierarchy  \n",
    "from comet_ml.query import Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CometExperimentManager:\n",
    "    def __init__(self, api_key: str = None, workspace: str = None):\n",
    "        self.api = API(api_key)\n",
    "        self.workspace = workspace\n",
    "\n",
    "    def get_all_artifacts_indexed(self):\n",
    "        \"\"\"Retrieve all artifacts and sort them by source experiment key.\"\"\"\n",
    "        filename = '/tmp/artifacts/artifacts_results.json'\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r') as file:\n",
    "                return json.load(file)\n",
    "            \n",
    "        artifacts = self.api.get_artifact_list(workspace=self.workspace)\n",
    "        experiment = start(workspace=self.workspace)\n",
    "        results = {}\n",
    "        for artifact_dict in artifacts['artifacts']:\n",
    "            id = artifact_dict['name']\n",
    "            try:\n",
    "                logged_artifact = experiment.get_artifact(id)\n",
    "                print(logged_artifact.source_experiment_key, id)\n",
    "                results[logged_artifact.source_experiment_key] = id\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving artifact {id}: {e}\")\n",
    "\n",
    "        # Save results to a file\n",
    "        with open(filename, \"w\") as file:\n",
    "            json.dump(results, file, indent=4)\n",
    "        experiment.end()\n",
    "        return results\n",
    "\n",
    "    def get_experiments_by_metrics(self, project_name: str = None, score_threshold: float = None, reward_threshold: float = None):\n",
    "        \"\"\"\n",
    "        Retrieve experiments for a project where the metric 'score' is less than\n",
    "        score_threshold and 'reward' is greater than or equal to reward_threshold.\n",
    "        \"\"\"\n",
    "        # experiments = self.api.query(self.workspace, project_name, Metric(\"score\") < score_threshold & Metric(\"reward\") >= reward_threshold)\n",
    "        experiments = self.api.query(self.workspace, project_name, Metric(\"score\") < score_threshold )\n",
    " \n",
    "        return experiments\n",
    "\n",
    "    def get_artifact_name(self, experiment: APIExperiment = None):\n",
    "        \"\"\"Retrieve the name of an artifact from an experiment.\"\"\"\n",
    "        artifacts = experiment.get_artifacts()\n",
    "        return artifacts[0]['artifact_name'] if artifacts else None\n",
    "\n",
    "    def download_and_run_artifact(self, artifact_name: str = None, seeds: list[int] = None):\n",
    "        \"\"\"\n",
    "        Download an artifact to '/tmp/artifacts/' and run PCTHierarchy.run_from_file\n",
    "        with the artifact filename, returning the score value for each run.\n",
    "        \"\"\"\n",
    "        download_path = f\"/tmp/artifacts/\"\n",
    "        os.makedirs(os.path.dirname(download_path), exist_ok=True)\n",
    " \n",
    "        full_path = os.path.join(download_path, artifact_name)\n",
    "        if os.path.exists(full_path):\n",
    "            pass\n",
    "        else:\n",
    "            experiment = start(workspace=self.workspace)\n",
    "            logged_artifact  = experiment.get_artifact(artifact_name)\n",
    "            # print(logged_artifact.source_experiment_key)\n",
    "            # local_artifact = logged_artifact.download(download_path)\n",
    "            logged_artifact.download(download_path)\n",
    "            # filename = f\"{local_artifact.download_local_path}{artifact_name}\"\n",
    "            experiment.end()\n",
    "    \n",
    "        rewards = []\n",
    "        for seed in seeds:\n",
    "            hierarchy, score = PCTHierarchy.run_from_file(full_path, seed=seed)\n",
    "            metrics = hierarchy.get_environment().get_metrics()\n",
    "            # print(f'Score={score:0.3f} {metrics}')\n",
    "            rewards.append(metrics['reward'])\n",
    "\n",
    "\n",
    "\n",
    "        return rewards\n",
    "\n",
    "    def get_original_metrics(self, experiment: APIExperiment = None):\n",
    "        \"\"\"Retrieve the metrics for an experiment.\"\"\"\n",
    "        metrics = {}\n",
    "        metrics['score'] = eval(experiment.get_metrics(\"score\")[0]['metricValue'])\n",
    "        hyperparameters = experiment.get_parameters_summary()\n",
    "\n",
    "        for param in hyperparameters:\n",
    "            if param['name'] == 'mode':\n",
    "                metrics['mode'] = param['valueCurrent']\n",
    "                break\n",
    "\n",
    "        metrics['name'] = experiment.name\n",
    "        return metrics\n",
    "\n",
    "    def run_experiments_and_record_results(self, project_name: str = None, experiments: list[APIExperiment] = None, artifact_results: dict = None, num_runs: int = 0, output_csv: str = None):\n",
    "        \"\"\"\n",
    "        Run each experiment for a given project a specified number of times and record the score and the\n",
    "        number of times the reward is 100, -100, or something else. Save results to a CSV file.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for experiment in experiments:\n",
    "            artifact_name = artifact_results[experiment.id]\n",
    "\n",
    "            metrics = self.get_original_metrics(experiment)\n",
    "\n",
    "            print(f\"Running experiment {experiment.id} in project {project_name} with artifact {artifact_name}\")\n",
    "            if not artifact_name:\n",
    "                continue\n",
    "\n",
    "            rewards = self.download_and_run_artifact(artifact_name, seeds=range(num_runs))\n",
    "            reward_counts = {'100': 0, '-100': 0, 'other': 0}\n",
    "\n",
    "            for reward in rewards:\n",
    "                if reward == 100:\n",
    "                    reward_counts['100'] += 1\n",
    "                elif reward == -100:\n",
    "                    reward_counts['-100'] += 1\n",
    "                else:\n",
    "                    reward_counts['other'] += 1\n",
    "            print(f\"Rewards: {reward_counts}\")\n",
    "            results.append({\n",
    "                'name': metrics['name'],\n",
    "                'score': round(metrics['score'], 5),\n",
    "                'mode': metrics['mode'],\n",
    "                'reward_100': reward_counts['100'],\n",
    "                'reward_-100': reward_counts['-100'],\n",
    "                'reward_other': reward_counts['other'],\n",
    "                'experiment_key': '/'.join((\"https://www.comet.com\", self.workspace, project_name, experiment.id)),\n",
    "                'artifact_name': artifact_name\n",
    "            })\n",
    "        # Sort results by 'reward_100' in descending order, then by 'reward_other' in descending order\n",
    "        results.sort(key=lambda x: (-x['reward_100'], -x['reward_other']))\n",
    "        # Save results to CSV\n",
    "        with open(output_csv, mode='w', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=['name', 'score', 'mode', 'reward_100', 'reward_-100', 'reward_other', 'experiment_key', 'artifact_name'])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered experiments: [<APIExperiment 'lunarlandercontinuous-v2/refinputs-smooth/319b8777136d4e059261d6e1582cded9'>, <APIExperiment 'lunarlandercontinuous-v2/refinputs-smooth/e6751db7179d4d6483ffd82985285f3e'>, <APIExperiment 'lunarlandercontinuous-v2/refinputs-smooth/39dfeda830bd4cf59759368a36bccb67'>, <APIExperiment 'lunarlandercontinuous-v2/refinputs-smooth/70d024308d024f24bcf4d9a7352ca775'>, <APIExperiment 'lunarlandercontinuous-v2/refinputs-smooth/bd30363071b74bc98582b51107cd38de'>, <APIExperiment 'lunarlandercontinuous-v2/refinputs-smooth/c888399ea415447d9c9954ca1f83f73d'>, <APIExperiment 'lunarlandercontinuous-v2/refinputs-smooth/620552456f29486fa2140e7afeeea053'>, <APIExperiment 'lunarlandercontinuous-v2/refinputs-smooth/5f14659ba5654ca4a7e894207f1b14e3'>, <APIExperiment 'lunarlandercontinuous-v2/refinputs-smooth/1d922993d9f7437693c6d66e2a4aa78b'>, <APIExperiment 'lunarlandercontinuous-v2/refinputs-smooth/873ef105d9004b9f81cff6caed888a5a'>, <APIExperiment 'lunarlandercontinuous-v2/refinputs-smooth/24798b73c782484d8e1b405f26260d2f'>, <APIExperiment 'lunarlandercontinuous-v2/refinputs-smooth/04a7b50f27484ec59e66c476ffd74dab'>, <APIExperiment 'lunarlandercontinuous-v2/refinputs-smooth/5dd88f8d7a8448718def12f01574d84a'>, <APIExperiment 'lunarlandercontinuous-v2/refinputs-smooth/47d2a0e0d59f4001ba27247a16a66c49'>, <APIExperiment 'lunarlandercontinuous-v2/refinputs-smooth/95089e886886457c8c4fb0e47bc2eecc'>]\n"
     ]
    }
   ],
   "source": [
    "#|gui\n",
    "# Initialize the manager\n",
    "workspace = 'lunarlandercontinuous-v2'\n",
    "project_name = 'refinputs-smooth'\n",
    "manager = CometExperimentManager(workspace=workspace)\n",
    "\n",
    "# Test get_all_artifacts_sorted\n",
    "artifact_results = manager.get_all_artifacts_indexed()\n",
    "# print(\"Artifacts sorted by source experiment key:\", artifacts)\n",
    "\n",
    "# Test get_experiments_by_metrics\n",
    "experiments = manager.get_experiments_by_metrics(project_name=project_name, score_threshold=0.05, reward_threshold=10.0)\n",
    "print(\"Filtered experiments:\", experiments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment 319b8777136d4e059261d6e1582cded9 in project refinputs-smooth with artifact ga-000.009-s001-1x6-m000-LL0061-1d7c4567db2419cd0699fc7dacd1e567.properties\n",
      "Rewards: {'100': 1, '-100': 1, 'other': 0}\n",
      "Running experiment e6751db7179d4d6483ffd82985285f3e in project refinputs-smooth with artifact ga-000.011-s002-1x6-m001-LL0062-0a9e1c620271e5d3e0a02bcab106a413-consolidated.properties\n",
      "Rewards: {'100': 1, '-100': 0, 'other': 1}\n",
      "Running experiment 39dfeda830bd4cf59759368a36bccb67 in project refinputs-smooth with artifact ga-000.008-s003-1x6-m001-LL0062-d72a0f7fcd4502d784e75d34d3b5b8de-consolidated.properties\n",
      "Rewards: {'100': 0, '-100': 2, 'other': 0}\n",
      "Running experiment 70d024308d024f24bcf4d9a7352ca775 in project refinputs-smooth with artifact ga-000.013-s003-1x6-m002-LL0063-8b5293fa0a14cc94337149e8dcda75c9.properties\n",
      "Rewards: {'100': 0, '-100': 2, 'other': 0}\n",
      "Running experiment bd30363071b74bc98582b51107cd38de in project refinputs-smooth with artifact ga-000.016-s002-1x6-m002-LL0063-195351a8259720c330b329205b58788d.properties\n",
      "Rewards: {'100': 0, '-100': 2, 'other': 0}\n",
      "Running experiment c888399ea415447d9c9954ca1f83f73d in project refinputs-smooth with artifact ga-000.027-s003-1x6-m003-LL0064-e2111229242563028825f0d81614b289.properties\n",
      "Rewards: {'100': 1, '-100': 1, 'other': 0}\n",
      "Running experiment 620552456f29486fa2140e7afeeea053 in project refinputs-smooth with artifact ga-000.002-s001-1x6-m003-LL0064-b7d8ffbd77713807f8bb8013d3adeb30.properties\n",
      "Rewards: {'100': 1, '-100': 1, 'other': 0}\n",
      "Running experiment 5f14659ba5654ca4a7e894207f1b14e3 in project refinputs-smooth with artifact ga-000.012-s001-1x6-m004-LL0065-7fa4fd3d76713bab132f34e4c195adc7-consolidated.properties\n",
      "Rewards: {'100': 1, '-100': 1, 'other': 0}\n",
      "Running experiment 1d922993d9f7437693c6d66e2a4aa78b in project refinputs-smooth with artifact ga-000.029-s002-1x6-m009-LL0069-4d682b51a4b8290142b8026171ec51da-consolidated.properties\n",
      "Rewards: {'100': 1, '-100': 0, 'other': 1}\n",
      "Running experiment 873ef105d9004b9f81cff6caed888a5a in project refinputs-smooth with artifact ga-000.025-s001-1x6-m009-LL0069-bf76f68ca626269b483e5651898ffe2c-consolidated.properties\n",
      "Rewards: {'100': 1, '-100': 1, 'other': 0}\n",
      "Running experiment 24798b73c782484d8e1b405f26260d2f in project refinputs-smooth with artifact ga-000.003-s003-1x6-m009-LL0069-ce0088e445f815398df02017ed2ab9ff-consolidated.properties\n",
      "Rewards: {'100': 1, '-100': 1, 'other': 0}\n",
      "Running experiment 04a7b50f27484ec59e66c476ffd74dab in project refinputs-smooth with artifact ga-000.001-s002-1x6-m011-LL0071-f2d83dfeddf17d1b316f4dd7180bb5b6-consolidated.properties\n",
      "Rewards: {'100': 0, '-100': 1, 'other': 1}\n",
      "Running experiment 5dd88f8d7a8448718def12f01574d84a in project refinputs-smooth with artifact ga-000.044-s002-1x6-m014-LL0074-e9f21b789827afb3b669f79043ad40a3-consolidated.properties\n",
      "Rewards: {'100': 0, '-100': 2, 'other': 0}\n",
      "Running experiment 47d2a0e0d59f4001ba27247a16a66c49 in project refinputs-smooth with artifact ga-000.047-s002-1x6-m017-LL0077-9ea6b48051cd9314a935ea62b6d6353f-consolidated.properties\n",
      "Rewards: {'100': 0, '-100': 2, 'other': 0}\n",
      "Running experiment 95089e886886457c8c4fb0e47bc2eecc in project refinputs-smooth with artifact ga-000.004-s001-1x6-m020-LL0080-357ef4ba877e2d577a72f9e4e6dc7847-consolidated.properties\n",
      "Rewards: {'100': 1, '-100': 1, 'other': 0}\n",
      "Results saved to /tmp/artifacts/experiment_results.csv\n"
     ]
    }
   ],
   "source": [
    "#|gui\n",
    "# import random\n",
    "\n",
    "# Test run_experiments_and_record_results\n",
    "if experiments:\n",
    "    output_csv = \"/tmp/artifacts/experiment_results.csv\"\n",
    "    manager.run_experiments_and_record_results(experiments=experiments, project_name=project_name, artifact_results=artifact_results, num_runs=2, output_csv=output_csv)\n",
    "    print(f\"Results saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
